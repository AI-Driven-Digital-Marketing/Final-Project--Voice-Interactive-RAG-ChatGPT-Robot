{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6580af",
   "metadata": {},
   "source": [
    "### RAG GPT\n",
    "Do not upload to github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273fdd5",
   "metadata": {},
   "source": [
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6fe728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a636f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71074\\anaconda3\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 10}},\n",
       " 'total_vector_count': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pinecone vector database, specify name and metadata, vector length\n",
    "import pinecone\n",
    "\n",
    "index_name = 'rag-training'\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=\"e55c8ed0-af09-4521-8a56-8b0776c0aa8c\",\n",
    "    environment=\"us-west4-gcp\"  # may be different, check at app.pinecone.io\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['source']}\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c8c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=2, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Taking the context below into consideration, answer the question. If the context not related, ignore it.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95f4b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mock interivew promot, still under working\n",
    "\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "# Define the conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly interviewer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Let's do mock interview! You ask me a question, then I answer your question, \n",
    "                                    you ask more follow up question or start a new question.\n",
    "                                    I will send you a job description and everything should be around that,\n",
    "                                    excepts behavior question.\"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Got you! Please send me your Job Description.\"},\n",
    "]\n",
    "\n",
    "# Continue the conversation\n",
    "def continue_conversation(conversation_history, user_message):\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=conversation_history\n",
    "    )\n",
    "\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    return assistant_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "936dd488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Here's my first question: Can you tell me about your experience in data visualization, statistical models, and data mining tools?\n"
     ]
    }
   ],
   "source": [
    "user_message = \"\"\"Minimum Qualifications\n",
    "\n",
    "One year of permanent service as a Data Analyst 1 OR bachelorâ€™s degree in data or computer science, informatics, economics, statistics, mathematics, or a related field, and three years of experience in data visualization, and statistical models and data mining tools.\n",
    "\n",
    "Duties Description\n",
    "\n",
    "The NYSIF Claims Department is seeking an experienced Data Analyst to lead the production of decision intelligence solutions for our Data and Innovation initiatives. The successful candidate will exhibit a high degree of flexibility, ability to learn new tools and methods of analysis, and have a strong commitment to performing high quality analysis.\n",
    "\n",
    "Responsibilities Include, But Are Not Limited To\n",
    "Produces business intelligence solutions by performing data extraction with tools such as Spotfire, SQL & R, including user-centric dashboards, predictive modeling, data analytics, and periodic reports\n",
    "Identify data patterns and trends, research new sources of data, and recommend solutions\n",
    "Devises methods for identifying data patterns, trend identification and root cause analysis\n",
    "Manage research projects and perform independent data analysis as required by Claims Department leadership and recommend improvements\n",
    "Conducts or coordinates tests to ensure that business intelligence solutions are consistent with defined needs\n",
    "Assesses data structures and how they can be used to produce desired reports\n",
    "Maintain life cycle of data solutions to ensure accuracy and integrity\n",
    "Assist Data Analyst 1 in the organization and prioritization of data requests\n",
    "Supervise lower-level staff\n",
    "Manage research projects and analyze data needed by Executive and NYSIF departments\n",
    "Assist the Data Analyst 1 in the organization and the prioritization of numerous data requests\"\"\"\n",
    "assistant_response = continue_conversation(conversation_history, user_message)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37c14635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's impressive! Can you give me an example of a data visualization project that you've led or worked on? How did you structure it and what were the results?\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Sure! I am proficient in Power BI, knowing machine learning algorithms like Random Forest, KNN and Linear Regression\"\n",
    "\n",
    "assistant_response = continue_conversation(conversation_history, user_message)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d896182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No worries, let's try a different question. Can you give me an example of a challenging situation you have faced at work, and how you approached and solved it?\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Nah sorry I cheated. Actually I know nothing about that.\"\n",
    "\n",
    "assistant_response = continue_conversation(conversation_history, user_message)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc6db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot continue this mock interview if you are going to be inappropriate and unprofessional. Please take this opportunity more seriously or we will have to end the session.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Sure! One day my colleage challenge my code so I beat him in the ass! Hell Yeah!\"\n",
    "\n",
    "assistant_response = continue_conversation(conversation_history, user_message)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8ee251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a joke for you: Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f36eee",
   "metadata": {},
   "source": [
    "#### Embedding text\n",
    "This cost credit of openai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3bd2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('output.json', 'r') as f:\n",
    "    # Load the JSON data from the file\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7947de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83370445a7dc4786b8c9beaa612aac64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "from time import sleep\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    meta_batch = data[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    # create embeddings (try-except added to avoid RateLimitError)\n",
    "    try:\n",
    "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "    except:\n",
    "        done = False\n",
    "        while not done:\n",
    "            sleep(5)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                done = True\n",
    "            except:\n",
    "                pass\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'text': x.get('text','NaN')\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91fd227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'HubSpot Glossary\\nLast updated: March 17, 2023\\n\\nAPPLIES TO:\\nAll products and plans\\nLearn definitions for common terms used in HubSpot tools. \\n\\nAccount & Setup\\nGDPR: the General Data Protection Regulation is an EU Regulation that replaced the 1995 EU Data Protection Directive. HubSpot provides settings for GDPR compliance.\\nIntegration: a connection between two apps, such as Slack and HubSpot. The user connects to the app, and the integration is the connection between HubSpot and the app.\\nMarketplace:\\nApp Marketplace: an online directory where you can download free and paid apps that integrate with HubSpot.\\nAsset Marketplace: an online directory where you can download free and paid modules, templates, and themes for use in marketing emails, landing pages, website pages, and blogs.\\nLimiting access: a way to separate your HubSpot tools and assets to specific users or teams.\\nPermission sets: pre-defined sets of user permissions. Use permission sets to ensure consistent permissions across your users.\\nTeams: groups of users in your account that can be used for organizational and reporting purposes.\\n',\n",
       " 'id': 'glossary_0'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeecf44",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59fd951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return enhanced prompt\n",
    "prompt = \"How Hubspot call 'a Sales email of multiple reports'?Anything related to it?\"\n",
    "prompt_with_rag = retrieve(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd30e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call API with enhance prompt, this  cost credits\n",
    "# you can also paste 'prompt_with_rag' to chatgpt website to test\n",
    "out = complete(prompt_with_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15469e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dashboard.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392106d",
   "metadata": {},
   "source": [
    "#### Mock Interview Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814cd4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
